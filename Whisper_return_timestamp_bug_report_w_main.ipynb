{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhavs10/scratchpad/blob/main/Whisper_return_timestamp_bug_report_w_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GEqvV-N1mE-A",
        "outputId": "1f37f212-1dde-4029-d1ae-1ceab4ba77c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-mnscvega\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-mnscvega\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit c836f77266be9ace47bff472f63caf71c0d11333\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (23.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev0) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.27.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.27.0.dev0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.27.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.27.0.dev0) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.27.0.dev0-py3-none-any.whl size=6464552 sha256=7a0729c4a22d582d421ad910e6129bb83027917be2a331579cf4198f11a23ba5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wf_heb9m/wheels/05/0a/97/64ae47c27ba95fae2cb5838e7b4b7247a34d4a8ba5f7092de2\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, xxhash, urllib3, multiprocess, responses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 huggingface-hub-0.12.0 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.2 transformers-4.27.0.dev0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Z5Z04TmlJ0"
      },
      "source": [
        "Just for future reference, this test was we are on `transformers 4.27.0.dev0` at the time of testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE18Ph1YpqdP",
        "outputId": "57d2b5f7-aa74-4206-e56c-cd9c6cc9b602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-13 08:10:14.258128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-13 08:10:17.547012: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-13 08:10:17.547203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-13 08:10:17.547239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/transformers/commands/env.py:52: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/transformers/commands/env.py:52: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "2023-02-13 08:10:24.943324: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\n",
            "Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n",
            "\n",
            "- `transformers` version: 4.27.0.dev0\n",
            "- Platform: Linux-5.10.147+-x86_64-with-glibc2.29\n",
            "- Python version: 3.8.10\n",
            "- Huggingface_hub version: 0.12.0\n",
            "- PyTorch version (GPU?): 1.13.1+cu116 (False)\n",
            "- Tensorflow version (GPU?): 2.11.0 (False)\n",
            "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
            "- Jax version: not installed\n",
            "- JaxLib version: not installed\n",
            "- Using GPU in script?: <fill in>\n",
            "- Using distributed or parallel set-up in script?: <fill in>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!transformers-cli env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jyTSHBmRmXIN"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\", generate_kwargs={\"task\": \"transcribe\", \"language\": \"<|de|>\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8EjiZTcnQTw"
      },
      "source": [
        "Loading the CV11 german split in streaming mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VtERRD1gnLHh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "cv11 = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"de\", streaming=True, split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0csN98nnfoy"
      },
      "source": [
        "Checking the first element to see if the transcription actually is `german`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sATQ7IynV9k",
        "outputId": "ac82e06c-84ef-44c0-dc37-d233ee5468c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 16082it [00:00, 49621.91it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'client_id': '0052c07533a6976233ad5926d950b523002c4d8cdd9ae8726dbfec385951bd22aa707a742c49afe20c7d6cb9515dbaddac5b4d6fe8ebddcfbec46a2d3180a3a1',\n",
              " 'path': 'common_voice_de_17922420.mp3',\n",
              " 'audio': {'path': 'common_voice_de_17922420.mp3',\n",
              "  'array': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
              "         -9.0749630e-12,  5.6385865e-09,  7.3282314e-09], dtype=float32),\n",
              "  'sampling_rate': 48000},\n",
              " 'sentence': 'Zieht euch bitte draußen die Schuhe aus.',\n",
              " 'up_votes': 2,\n",
              " 'down_votes': 0,\n",
              " 'age': '',\n",
              " 'gender': '',\n",
              " 'accent': '',\n",
              " 'locale': 'de',\n",
              " 'segment': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "next(iter(cv11))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PArW-FFSnoh3"
      },
      "source": [
        "Looks pretty german to me. In case you missed it, look at the `sentence` key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcHjiHdOoIxH"
      },
      "source": [
        "Now, let's create a test sample from this sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0OmxkxOneLG",
        "outputId": "428b6c62-ca2c-4869-a1b4-f8ea3aede20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 16082it [00:00, 57288.45it/s]\n",
            "Reading metadata...: 16082it [00:00, 56202.61it/s]\n"
          ]
        }
      ],
      "source": [
        "test_sample = {\"raw\": next(iter(cv11))[\"audio\"][\"array\"],\n",
        "               \"sampling_rate\": next(iter(cv11))[\"audio\"][\"sampling_rate\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVKM0fBpoDGX",
        "outputId": "ee79e29b-6038-4b63-cf91-ad67f0be8e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1268: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' Zieht euch bitte draußen die Schuhe aus.'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "pipe(test_sample, chunk_length_s=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTLn3Ea-odxJ"
      },
      "source": [
        "As seen above the pipeline simply refuses to accept the defined `generate_kwargs`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpRksb4koosY"
      },
      "source": [
        "Let's try with `return_timestamps=True` and see how the response to this is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG9vtmX3ozdt",
        "outputId": "cad71bac-f239-4a50-c05e-5f460f44d4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 16082it [00:00, 22873.59it/s]\n",
            "Reading metadata...: 16082it [00:00, 20316.21it/s]\n"
          ]
        }
      ],
      "source": [
        "test_sample = {\"raw\": next(iter(cv11))[\"audio\"][\"array\"],\n",
        "               \"sampling_rate\": next(iter(cv11))[\"audio\"][\"sampling_rate\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb3WR8ikoX0D",
        "outputId": "020c7a61-1e0a-45ea-b184-a2104899acf2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' Zieht euch bitte draußen die Schuhe aus.',\n",
              " 'chunks': [{'text': ' Zieht euch bitte draußen die Schuhe aus.',\n",
              "   'timestamp': (0.0, 3.0)}]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "pipe(test_sample, return_timestamps=True, chunk_length_s=30, stride_length_s=[6,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmZFYa7co9xF"
      },
      "source": [
        "Now, the pipeline works, however, completely refuses to accept the `generate_kwargs` i.e. doesn't transcribe in german."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lynz0Gythzw"
      },
      "source": [
        "Attempt 2: Let's remove the `generate_kwargs` and try with `forced_decoder_ids`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jZft-iG_tpop"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ic1ukynotxQm"
      },
      "outputs": [],
      "source": [
        "pipe.model.config.forced_decoder_ids = (\n",
        "    pipe.tokenizer.get_decoder_prompt_ids(\n",
        "        language=\"de\", task=\"transcribe\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx3DM1vst5f8",
        "outputId": "fad19585-d4b2-40b9-b911-77ff047eccfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 16082it [00:00, 45812.71it/s]\n",
            "Reading metadata...: 16082it [00:00, 55874.44it/s]\n"
          ]
        }
      ],
      "source": [
        "test_sample = {\"raw\": next(iter(cv11))[\"audio\"][\"array\"],\n",
        "               \"sampling_rate\": next(iter(cv11))[\"audio\"][\"sampling_rate\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6gq6mK2uEvQ",
        "outputId": "3ec69fde-ac0e-4090-fdcf-dcdbafce5312"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' Zieht euch bitte draußen die Schuhe aus.'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "pipe(test_sample, chunk_length_s=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlMv7eobvG94"
      },
      "source": [
        "Okay, that seem to work perfectly. Let's try and use the same with `return_timestamps=True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfHZ85ivvSfH",
        "outputId": "ace486e2-8023-4096-817b-6f90f8452851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 16082it [00:00, 58640.02it/s]\n",
            "Reading metadata...: 16082it [00:00, 49153.46it/s]\n"
          ]
        }
      ],
      "source": [
        "test_sample = {\"raw\": next(iter(cv11))[\"audio\"][\"array\"],\n",
        "               \"sampling_rate\": next(iter(cv11))[\"audio\"][\"sampling_rate\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2jPkVkCuJ3m",
        "outputId": "d5251580-e999-4257-cf57-61a2976ba0df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' Zieht euch bitte draußen die Schuhe aus.',\n",
              " 'chunks': [{'text': ' Zieht euch bitte draußen die Schuhe aus.',\n",
              "   'timestamp': (0.0, 3.0)}]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "pipe(test_sample, return_timestamps=True, chunk_length_s=30, stride_length_s=[6,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffz29sIRvY3t"
      },
      "source": [
        "Um, again, `translates` to english."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlJaxpB+u0QtQagRkQ5wxy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}