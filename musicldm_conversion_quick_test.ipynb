{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIABg0ZZ2dSiHrAfonN8bt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhavs10/scratchpad/blob/main/musicldm_conversion_quick_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "6bkBEUynGILW",
        "outputId": "c5548f8d-cb82-43e8-c401-bc3d15806dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers) (4.6.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=50c0e8e62af1049ceb456a791dd09b79a7031c8ddd8e5616fe578c143e7df1f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install transformers diffusers huggingface_hub omegaconf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/reach-vb/musicldm-test/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvyVO94nQHrr",
        "outputId": "bdd89210-5f34-4924-a63b-d207b6798e81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'musicldm-test'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), 1.23 KiB | 210.00 KiB/s, done.\n",
            "Filtering content: 100% (4/4), 7.08 GiB | 33.07 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tmusicldm-ckpt.ckpt\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile convert.py\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2023 The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Conversion script for the AudioLDM checkpoints.\"\"\"\n",
        "\n",
        "import argparse\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    ClapTextConfig,\n",
        "    ClapTextModelWithProjection,\n",
        "    SpeechT5HifiGan,\n",
        "    SpeechT5HifiGanConfig,\n",
        ")\n",
        "\n",
        "from diffusers import (\n",
        "    AudioLDMPipeline,\n",
        "    AutoencoderKL,\n",
        "    DDIMScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    PNDMScheduler,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "from diffusers.utils import is_omegaconf_available, is_safetensors_available\n",
        "from diffusers.utils.import_utils import BACKENDS_MAPPING\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.shave_segments\n",
        "def shave_segments(path, n_shave_prefix_segments=1):\n",
        "    \"\"\"\n",
        "    Removes segments. Positive values shave the first segments, negative shave the last segments.\n",
        "    \"\"\"\n",
        "    if n_shave_prefix_segments >= 0:\n",
        "        return \".\".join(path.split(\".\")[n_shave_prefix_segments:])\n",
        "    else:\n",
        "        return \".\".join(path.split(\".\")[:n_shave_prefix_segments])\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.renew_resnet_paths\n",
        "def renew_resnet_paths(old_list, n_shave_prefix_segments=0):\n",
        "    \"\"\"\n",
        "    Updates paths inside resnets to the new naming scheme (local renaming)\n",
        "    \"\"\"\n",
        "    mapping = []\n",
        "    for old_item in old_list:\n",
        "        new_item = old_item.replace(\"in_layers.0\", \"norm1\")\n",
        "        new_item = new_item.replace(\"in_layers.2\", \"conv1\")\n",
        "\n",
        "        new_item = new_item.replace(\"out_layers.0\", \"norm2\")\n",
        "        new_item = new_item.replace(\"out_layers.3\", \"conv2\")\n",
        "\n",
        "        new_item = new_item.replace(\"emb_layers.1\", \"time_emb_proj\")\n",
        "        new_item = new_item.replace(\"skip_connection\", \"conv_shortcut\")\n",
        "\n",
        "        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n",
        "\n",
        "        mapping.append({\"old\": old_item, \"new\": new_item})\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.renew_vae_resnet_paths\n",
        "def renew_vae_resnet_paths(old_list, n_shave_prefix_segments=0):\n",
        "    \"\"\"\n",
        "    Updates paths inside resnets to the new naming scheme (local renaming)\n",
        "    \"\"\"\n",
        "    mapping = []\n",
        "    for old_item in old_list:\n",
        "        new_item = old_item\n",
        "\n",
        "        new_item = new_item.replace(\"nin_shortcut\", \"conv_shortcut\")\n",
        "        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n",
        "\n",
        "        mapping.append({\"old\": old_item, \"new\": new_item})\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.renew_attention_paths\n",
        "def renew_attention_paths(old_list):\n",
        "    \"\"\"\n",
        "    Updates paths inside attentions to the new naming scheme (local renaming)\n",
        "    \"\"\"\n",
        "    mapping = []\n",
        "    for old_item in old_list:\n",
        "        new_item = old_item\n",
        "\n",
        "        #         new_item = new_item.replace('norm.weight', 'group_norm.weight')\n",
        "        #         new_item = new_item.replace('norm.bias', 'group_norm.bias')\n",
        "\n",
        "        #         new_item = new_item.replace('proj_out.weight', 'proj_attn.weight')\n",
        "        #         new_item = new_item.replace('proj_out.bias', 'proj_attn.bias')\n",
        "\n",
        "        #         new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n",
        "\n",
        "        mapping.append({\"old\": old_item, \"new\": new_item})\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.renew_vae_attention_paths\n",
        "def renew_vae_attention_paths(old_list, n_shave_prefix_segments=0):\n",
        "    \"\"\"\n",
        "    Updates paths inside attentions to the new naming scheme (local renaming)\n",
        "    \"\"\"\n",
        "    mapping = []\n",
        "    for old_item in old_list:\n",
        "        new_item = old_item\n",
        "\n",
        "        new_item = new_item.replace(\"norm.weight\", \"group_norm.weight\")\n",
        "        new_item = new_item.replace(\"norm.bias\", \"group_norm.bias\")\n",
        "\n",
        "        new_item = new_item.replace(\"q.weight\", \"query.weight\")\n",
        "        new_item = new_item.replace(\"q.bias\", \"query.bias\")\n",
        "\n",
        "        new_item = new_item.replace(\"k.weight\", \"key.weight\")\n",
        "        new_item = new_item.replace(\"k.bias\", \"key.bias\")\n",
        "\n",
        "        new_item = new_item.replace(\"v.weight\", \"value.weight\")\n",
        "        new_item = new_item.replace(\"v.bias\", \"value.bias\")\n",
        "\n",
        "        new_item = new_item.replace(\"proj_out.weight\", \"proj_attn.weight\")\n",
        "        new_item = new_item.replace(\"proj_out.bias\", \"proj_attn.bias\")\n",
        "\n",
        "        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n",
        "\n",
        "        mapping.append({\"old\": old_item, \"new\": new_item})\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.assign_to_checkpoint\n",
        "def assign_to_checkpoint(\n",
        "    paths, checkpoint, old_checkpoint, attention_paths_to_split=None, additional_replacements=None, config=None\n",
        "):\n",
        "    \"\"\"\n",
        "    This does the final conversion step: take locally converted weights and apply a global renaming to them. It splits\n",
        "    attention layers, and takes into account additional replacements that may arise.\n",
        "\n",
        "    Assigns the weights to the new checkpoint.\n",
        "    \"\"\"\n",
        "    assert isinstance(paths, list), \"Paths should be a list of dicts containing 'old' and 'new' keys.\"\n",
        "\n",
        "    # Splits the attention layers into three variables.\n",
        "    if attention_paths_to_split is not None:\n",
        "        for path, path_map in attention_paths_to_split.items():\n",
        "            old_tensor = old_checkpoint[path]\n",
        "            channels = old_tensor.shape[0] // 3\n",
        "\n",
        "            target_shape = (-1, channels) if len(old_tensor.shape) == 3 else (-1)\n",
        "\n",
        "            num_heads = old_tensor.shape[0] // config[\"num_head_channels\"] // 3\n",
        "\n",
        "            old_tensor = old_tensor.reshape((num_heads, 3 * channels // num_heads) + old_tensor.shape[1:])\n",
        "            query, key, value = old_tensor.split(channels // num_heads, dim=1)\n",
        "\n",
        "            checkpoint[path_map[\"query\"]] = query.reshape(target_shape)\n",
        "            checkpoint[path_map[\"key\"]] = key.reshape(target_shape)\n",
        "            checkpoint[path_map[\"value\"]] = value.reshape(target_shape)\n",
        "\n",
        "    for path in paths:\n",
        "        new_path = path[\"new\"]\n",
        "\n",
        "        # These have already been assigned\n",
        "        if attention_paths_to_split is not None and new_path in attention_paths_to_split:\n",
        "            continue\n",
        "\n",
        "        # Global renaming happens here\n",
        "        new_path = new_path.replace(\"middle_block.0\", \"mid_block.resnets.0\")\n",
        "        new_path = new_path.replace(\"middle_block.1\", \"mid_block.attentions.0\")\n",
        "        new_path = new_path.replace(\"middle_block.2\", \"mid_block.resnets.1\")\n",
        "\n",
        "        if additional_replacements is not None:\n",
        "            for replacement in additional_replacements:\n",
        "                new_path = new_path.replace(replacement[\"old\"], replacement[\"new\"])\n",
        "\n",
        "        # proj_attn.weight has to be converted from conv 1D to linear\n",
        "        if \"proj_attn.weight\" in new_path:\n",
        "            checkpoint[new_path] = old_checkpoint[path[\"old\"]][:, :, 0]\n",
        "        else:\n",
        "            checkpoint[new_path] = old_checkpoint[path[\"old\"]]\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.conv_attn_to_linear\n",
        "def conv_attn_to_linear(checkpoint):\n",
        "    keys = list(checkpoint.keys())\n",
        "    attn_keys = [\"query.weight\", \"key.weight\", \"value.weight\"]\n",
        "    for key in keys:\n",
        "        if \".\".join(key.split(\".\")[-2:]) in attn_keys:\n",
        "            if checkpoint[key].ndim > 2:\n",
        "                checkpoint[key] = checkpoint[key][:, :, 0, 0]\n",
        "        elif \"proj_attn.weight\" in key:\n",
        "            if checkpoint[key].ndim > 2:\n",
        "                checkpoint[key] = checkpoint[key][:, :, 0]\n",
        "\n",
        "\n",
        "def create_unet_diffusers_config(original_config, image_size: int):\n",
        "    \"\"\"\n",
        "    Creates a UNet config for diffusers based on the config of the original AudioLDM model.\n",
        "    \"\"\"\n",
        "    unet_params = original_config.model.params.unet_config.params\n",
        "    vae_params = original_config.model.params.first_stage_config.params.ddconfig\n",
        "\n",
        "    block_out_channels = [unet_params.model_channels * mult for mult in unet_params.channel_mult]\n",
        "\n",
        "    down_block_types = []\n",
        "    resolution = 1\n",
        "    for i in range(len(block_out_channels)):\n",
        "        block_type = \"CrossAttnDownBlock2D\" if resolution in unet_params.attention_resolutions else \"DownBlock2D\"\n",
        "        down_block_types.append(block_type)\n",
        "        if i != len(block_out_channels) - 1:\n",
        "            resolution *= 2\n",
        "\n",
        "    up_block_types = []\n",
        "    for i in range(len(block_out_channels)):\n",
        "        block_type = \"CrossAttnUpBlock2D\" if resolution in unet_params.attention_resolutions else \"UpBlock2D\"\n",
        "        up_block_types.append(block_type)\n",
        "        resolution //= 2\n",
        "\n",
        "    vae_scale_factor = 2 ** (len(vae_params.ch_mult) - 1)\n",
        "\n",
        "    cross_attention_dim = (\n",
        "        unet_params.cross_attention_dim if \"cross_attention_dim\" in unet_params else block_out_channels\n",
        "    )\n",
        "\n",
        "    class_embed_type = \"simple_projection\" if \"extra_film_condition_dim\" in unet_params else None\n",
        "    projection_class_embeddings_input_dim = (\n",
        "        unet_params.extra_film_condition_dim if \"extra_film_condition_dim\" in unet_params else None\n",
        "    )\n",
        "    class_embeddings_concat = unet_params.extra_film_use_concat if \"extra_film_use_concat\" in unet_params else None\n",
        "\n",
        "    config = {\n",
        "        \"sample_size\": image_size // vae_scale_factor,\n",
        "        \"in_channels\": unet_params.in_channels,\n",
        "        \"out_channels\": unet_params.out_channels,\n",
        "        \"down_block_types\": tuple(down_block_types),\n",
        "        \"up_block_types\": tuple(up_block_types),\n",
        "        \"block_out_channels\": tuple(block_out_channels),\n",
        "        \"layers_per_block\": unet_params.num_res_blocks,\n",
        "        \"cross_attention_dim\": cross_attention_dim,\n",
        "        \"class_embed_type\": class_embed_type,\n",
        "        \"projection_class_embeddings_input_dim\": projection_class_embeddings_input_dim,\n",
        "        \"class_embeddings_concat\": class_embeddings_concat,\n",
        "    }\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "# Adapted from diffusers.pipelines.stable_diffusion.convert_from_ckpt.create_vae_diffusers_config\n",
        "def create_vae_diffusers_config(original_config, checkpoint, image_size: int):\n",
        "    \"\"\"\n",
        "    Creates a VAE config for diffusers based on the config of the original AudioLDM model. Compared to the original\n",
        "    Stable Diffusion conversion, this function passes a *learnt* VAE scaling factor to the diffusers VAE.\n",
        "    \"\"\"\n",
        "    vae_params = original_config.model.params.first_stage_config.params.ddconfig\n",
        "    _ = original_config.model.params.first_stage_config.params.embed_dim\n",
        "\n",
        "    block_out_channels = [vae_params.ch * mult for mult in vae_params.ch_mult]\n",
        "    down_block_types = [\"DownEncoderBlock2D\"] * len(block_out_channels)\n",
        "    up_block_types = [\"UpDecoderBlock2D\"] * len(block_out_channels)\n",
        "\n",
        "    scaling_factor = checkpoint[\"scale_factor\"] if \"scale_by_std\" in original_config.model.params else 0.18215\n",
        "\n",
        "    config = {\n",
        "        \"sample_size\": image_size,\n",
        "        \"in_channels\": vae_params.in_channels,\n",
        "        \"out_channels\": vae_params.out_ch,\n",
        "        \"down_block_types\": tuple(down_block_types),\n",
        "        \"up_block_types\": tuple(up_block_types),\n",
        "        \"block_out_channels\": tuple(block_out_channels),\n",
        "        \"latent_channels\": vae_params.z_channels,\n",
        "        \"layers_per_block\": vae_params.num_res_blocks,\n",
        "        \"scaling_factor\": float(scaling_factor),\n",
        "    }\n",
        "    return config\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.create_diffusers_schedular\n",
        "def create_diffusers_schedular(original_config):\n",
        "    schedular = DDIMScheduler(\n",
        "        num_train_timesteps=original_config.model.params.timesteps,\n",
        "        beta_start=original_config.model.params.linear_start,\n",
        "        beta_end=original_config.model.params.linear_end,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "    )\n",
        "    return schedular\n",
        "\n",
        "\n",
        "# Adapted from diffusers.pipelines.stable_diffusion.convert_from_ckpt.convert_ldm_unet_checkpoint\n",
        "def convert_ldm_unet_checkpoint(checkpoint, config, path=None, extract_ema=False):\n",
        "    \"\"\"\n",
        "    Takes a state dict and a config, and returns a converted checkpoint. Compared to the original Stable Diffusion\n",
        "    conversion, this function additionally converts the learnt film embedding linear layer.\n",
        "    \"\"\"\n",
        "\n",
        "    # extract state_dict for UNet\n",
        "    unet_state_dict = {}\n",
        "    keys = list(checkpoint.keys())\n",
        "\n",
        "    unet_key = \"model.diffusion_model.\"\n",
        "    # at least a 100 parameters have to start with `model_ema` in order for the checkpoint to be EMA\n",
        "    if sum(k.startswith(\"model_ema\") for k in keys) > 100 and extract_ema:\n",
        "        print(f\"Checkpoint {path} has both EMA and non-EMA weights.\")\n",
        "        print(\n",
        "            \"In this conversion only the EMA weights are extracted. If you want to instead extract the non-EMA\"\n",
        "            \" weights (useful to continue fine-tuning), please make sure to remove the `--extract_ema` flag.\"\n",
        "        )\n",
        "        for key in keys:\n",
        "            if key.startswith(\"model.diffusion_model\"):\n",
        "                flat_ema_key = \"model_ema.\" + \"\".join(key.split(\".\")[1:])\n",
        "                unet_state_dict[key.replace(unet_key, \"\")] = checkpoint.pop(flat_ema_key)\n",
        "    else:\n",
        "        if sum(k.startswith(\"model_ema\") for k in keys) > 100:\n",
        "            print(\n",
        "                \"In this conversion only the non-EMA weights are extracted. If you want to instead extract the EMA\"\n",
        "                \" weights (usually better for inference), please make sure to add the `--extract_ema` flag.\"\n",
        "            )\n",
        "\n",
        "        for key in keys:\n",
        "            if key.startswith(unet_key):\n",
        "                unet_state_dict[key.replace(unet_key, \"\")] = checkpoint.pop(key)\n",
        "\n",
        "    new_checkpoint = {}\n",
        "\n",
        "    new_checkpoint[\"time_embedding.linear_1.weight\"] = unet_state_dict[\"time_embed.0.weight\"]\n",
        "    new_checkpoint[\"time_embedding.linear_1.bias\"] = unet_state_dict[\"time_embed.0.bias\"]\n",
        "    new_checkpoint[\"time_embedding.linear_2.weight\"] = unet_state_dict[\"time_embed.2.weight\"]\n",
        "    new_checkpoint[\"time_embedding.linear_2.bias\"] = unet_state_dict[\"time_embed.2.bias\"]\n",
        "\n",
        "    new_checkpoint[\"class_embedding.weight\"] = unet_state_dict[\"film_emb.weight\"]\n",
        "    new_checkpoint[\"class_embedding.bias\"] = unet_state_dict[\"film_emb.bias\"]\n",
        "\n",
        "    new_checkpoint[\"conv_in.weight\"] = unet_state_dict[\"input_blocks.0.0.weight\"]\n",
        "    new_checkpoint[\"conv_in.bias\"] = unet_state_dict[\"input_blocks.0.0.bias\"]\n",
        "\n",
        "    new_checkpoint[\"conv_norm_out.weight\"] = unet_state_dict[\"out.0.weight\"]\n",
        "    new_checkpoint[\"conv_norm_out.bias\"] = unet_state_dict[\"out.0.bias\"]\n",
        "    new_checkpoint[\"conv_out.weight\"] = unet_state_dict[\"out.2.weight\"]\n",
        "    new_checkpoint[\"conv_out.bias\"] = unet_state_dict[\"out.2.bias\"]\n",
        "\n",
        "    # Retrieves the keys for the input blocks only\n",
        "    num_input_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"input_blocks\" in layer})\n",
        "    input_blocks = {\n",
        "        layer_id: [key for key in unet_state_dict if f\"input_blocks.{layer_id}\" in key]\n",
        "        for layer_id in range(num_input_blocks)\n",
        "    }\n",
        "\n",
        "    # Retrieves the keys for the middle blocks only\n",
        "    num_middle_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"middle_block\" in layer})\n",
        "    middle_blocks = {\n",
        "        layer_id: [key for key in unet_state_dict if f\"middle_block.{layer_id}\" in key]\n",
        "        for layer_id in range(num_middle_blocks)\n",
        "    }\n",
        "\n",
        "    # Retrieves the keys for the output blocks only\n",
        "    num_output_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"output_blocks\" in layer})\n",
        "    output_blocks = {\n",
        "        layer_id: [key for key in unet_state_dict if f\"output_blocks.{layer_id}\" in key]\n",
        "        for layer_id in range(num_output_blocks)\n",
        "    }\n",
        "\n",
        "    for i in range(1, num_input_blocks):\n",
        "        block_id = (i - 1) // (config[\"layers_per_block\"] + 1)\n",
        "        layer_in_block_id = (i - 1) % (config[\"layers_per_block\"] + 1)\n",
        "\n",
        "        resnets = [\n",
        "            key for key in input_blocks[i] if f\"input_blocks.{i}.0\" in key and f\"input_blocks.{i}.0.op\" not in key\n",
        "        ]\n",
        "        attentions = [key for key in input_blocks[i] if f\"input_blocks.{i}.1\" in key]\n",
        "\n",
        "        if f\"input_blocks.{i}.0.op.weight\" in unet_state_dict:\n",
        "            new_checkpoint[f\"down_blocks.{block_id}.downsamplers.0.conv.weight\"] = unet_state_dict.pop(\n",
        "                f\"input_blocks.{i}.0.op.weight\"\n",
        "            )\n",
        "            new_checkpoint[f\"down_blocks.{block_id}.downsamplers.0.conv.bias\"] = unet_state_dict.pop(\n",
        "                f\"input_blocks.{i}.0.op.bias\"\n",
        "            )\n",
        "\n",
        "        paths = renew_resnet_paths(resnets)\n",
        "        meta_path = {\"old\": f\"input_blocks.{i}.0\", \"new\": f\"down_blocks.{block_id}.resnets.{layer_in_block_id}\"}\n",
        "        assign_to_checkpoint(\n",
        "            paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n",
        "        )\n",
        "\n",
        "        if len(attentions):\n",
        "            paths = renew_attention_paths(attentions)\n",
        "            meta_path = {\"old\": f\"input_blocks.{i}.1\", \"new\": f\"down_blocks.{block_id}.attentions.{layer_in_block_id}\"}\n",
        "            assign_to_checkpoint(\n",
        "                paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n",
        "            )\n",
        "\n",
        "    resnet_0 = middle_blocks[0]\n",
        "    attentions = middle_blocks[1]\n",
        "    resnet_1 = middle_blocks[2]\n",
        "\n",
        "    resnet_0_paths = renew_resnet_paths(resnet_0)\n",
        "    assign_to_checkpoint(resnet_0_paths, new_checkpoint, unet_state_dict, config=config)\n",
        "\n",
        "    resnet_1_paths = renew_resnet_paths(resnet_1)\n",
        "    assign_to_checkpoint(resnet_1_paths, new_checkpoint, unet_state_dict, config=config)\n",
        "\n",
        "    attentions_paths = renew_attention_paths(attentions)\n",
        "    meta_path = {\"old\": \"middle_block.1\", \"new\": \"mid_block.attentions.0\"}\n",
        "    assign_to_checkpoint(\n",
        "        attentions_paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n",
        "    )\n",
        "\n",
        "    for i in range(num_output_blocks):\n",
        "        block_id = i // (config[\"layers_per_block\"] + 1)\n",
        "        layer_in_block_id = i % (config[\"layers_per_block\"] + 1)\n",
        "        output_block_layers = [shave_segments(name, 2) for name in output_blocks[i]]\n",
        "        output_block_list = {}\n",
        "\n",
        "        for layer in output_block_layers:\n",
        "            layer_id, layer_name = layer.split(\".\")[0], shave_segments(layer, 1)\n",
        "            if layer_id in output_block_list:\n",
        "                output_block_list[layer_id].append(layer_name)\n",
        "            else:\n",
        "                output_block_list[layer_id] = [layer_name]\n",
        "\n",
        "        if len(output_block_list) > 1:\n",
        "            resnets = [key for key in output_blocks[i] if f\"output_blocks.{i}.0\" in key]\n",
        "            attentions = [key for key in output_blocks[i] if f\"output_blocks.{i}.1\" in key]\n",
        "\n",
        "            resnet_0_paths = renew_resnet_paths(resnets)\n",
        "            paths = renew_resnet_paths(resnets)\n",
        "\n",
        "            meta_path = {\"old\": f\"output_blocks.{i}.0\", \"new\": f\"up_blocks.{block_id}.resnets.{layer_in_block_id}\"}\n",
        "            assign_to_checkpoint(\n",
        "                paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n",
        "            )\n",
        "\n",
        "            output_block_list = {k: sorted(v) for k, v in output_block_list.items()}\n",
        "            if [\"conv.bias\", \"conv.weight\"] in output_block_list.values():\n",
        "                index = list(output_block_list.values()).index([\"conv.bias\", \"conv.weight\"])\n",
        "                new_checkpoint[f\"up_blocks.{block_id}.upsamplers.0.conv.weight\"] = unet_state_dict[\n",
        "                    f\"output_blocks.{i}.{index}.conv.weight\"\n",
        "                ]\n",
        "                new_checkpoint[f\"up_blocks.{block_id}.upsamplers.0.conv.bias\"] = unet_state_dict[\n",
        "                    f\"output_blocks.{i}.{index}.conv.bias\"\n",
        "                ]\n",
        "\n",
        "                # Clear attentions as they have been attributed above.\n",
        "                if len(attentions) == 2:\n",
        "                    attentions = []\n",
        "\n",
        "            if len(attentions):\n",
        "                paths = renew_attention_paths(attentions)\n",
        "                meta_path = {\n",
        "                    \"old\": f\"output_blocks.{i}.1\",\n",
        "                    \"new\": f\"up_blocks.{block_id}.attentions.{layer_in_block_id}\",\n",
        "                }\n",
        "                assign_to_checkpoint(\n",
        "                    paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config\n",
        "                )\n",
        "        else:\n",
        "            resnet_0_paths = renew_resnet_paths(output_block_layers, n_shave_prefix_segments=1)\n",
        "            for path in resnet_0_paths:\n",
        "                old_path = \".\".join([\"output_blocks\", str(i), path[\"old\"]])\n",
        "                new_path = \".\".join([\"up_blocks\", str(block_id), \"resnets\", str(layer_in_block_id), path[\"new\"]])\n",
        "\n",
        "                new_checkpoint[new_path] = unet_state_dict[old_path]\n",
        "\n",
        "    return new_checkpoint\n",
        "\n",
        "\n",
        "# Copied from diffusers.pipelines.stable_diffusion.convert_from_ckpt.convert_ldm_vae_checkpoint\n",
        "def convert_ldm_vae_checkpoint(checkpoint, config):\n",
        "    # extract state dict for VAE\n",
        "    vae_state_dict = {}\n",
        "    vae_key = \"first_stage_model.\"\n",
        "    keys = list(checkpoint.keys())\n",
        "    for key in keys:\n",
        "        if key.startswith(vae_key):\n",
        "            vae_state_dict[key.replace(vae_key, \"\")] = checkpoint.get(key)\n",
        "\n",
        "    new_checkpoint = {}\n",
        "\n",
        "    new_checkpoint[\"encoder.conv_in.weight\"] = vae_state_dict[\"encoder.conv_in.weight\"]\n",
        "    new_checkpoint[\"encoder.conv_in.bias\"] = vae_state_dict[\"encoder.conv_in.bias\"]\n",
        "    new_checkpoint[\"encoder.conv_out.weight\"] = vae_state_dict[\"encoder.conv_out.weight\"]\n",
        "    new_checkpoint[\"encoder.conv_out.bias\"] = vae_state_dict[\"encoder.conv_out.bias\"]\n",
        "    new_checkpoint[\"encoder.conv_norm_out.weight\"] = vae_state_dict[\"encoder.norm_out.weight\"]\n",
        "    new_checkpoint[\"encoder.conv_norm_out.bias\"] = vae_state_dict[\"encoder.norm_out.bias\"]\n",
        "\n",
        "    new_checkpoint[\"decoder.conv_in.weight\"] = vae_state_dict[\"decoder.conv_in.weight\"]\n",
        "    new_checkpoint[\"decoder.conv_in.bias\"] = vae_state_dict[\"decoder.conv_in.bias\"]\n",
        "    new_checkpoint[\"decoder.conv_out.weight\"] = vae_state_dict[\"decoder.conv_out.weight\"]\n",
        "    new_checkpoint[\"decoder.conv_out.bias\"] = vae_state_dict[\"decoder.conv_out.bias\"]\n",
        "    new_checkpoint[\"decoder.conv_norm_out.weight\"] = vae_state_dict[\"decoder.norm_out.weight\"]\n",
        "    new_checkpoint[\"decoder.conv_norm_out.bias\"] = vae_state_dict[\"decoder.norm_out.bias\"]\n",
        "\n",
        "    new_checkpoint[\"quant_conv.weight\"] = vae_state_dict[\"quant_conv.weight\"]\n",
        "    new_checkpoint[\"quant_conv.bias\"] = vae_state_dict[\"quant_conv.bias\"]\n",
        "    new_checkpoint[\"post_quant_conv.weight\"] = vae_state_dict[\"post_quant_conv.weight\"]\n",
        "    new_checkpoint[\"post_quant_conv.bias\"] = vae_state_dict[\"post_quant_conv.bias\"]\n",
        "\n",
        "    # Retrieves the keys for the encoder down blocks only\n",
        "    num_down_blocks = len({\".\".join(layer.split(\".\")[:3]) for layer in vae_state_dict if \"encoder.down\" in layer})\n",
        "    down_blocks = {\n",
        "        layer_id: [key for key in vae_state_dict if f\"down.{layer_id}\" in key] for layer_id in range(num_down_blocks)\n",
        "    }\n",
        "\n",
        "    # Retrieves the keys for the decoder up blocks only\n",
        "    num_up_blocks = len({\".\".join(layer.split(\".\")[:3]) for layer in vae_state_dict if \"decoder.up\" in layer})\n",
        "    up_blocks = {\n",
        "        layer_id: [key for key in vae_state_dict if f\"up.{layer_id}\" in key] for layer_id in range(num_up_blocks)\n",
        "    }\n",
        "\n",
        "    for i in range(num_down_blocks):\n",
        "        resnets = [key for key in down_blocks[i] if f\"down.{i}\" in key and f\"down.{i}.downsample\" not in key]\n",
        "\n",
        "        if f\"encoder.down.{i}.downsample.conv.weight\" in vae_state_dict:\n",
        "            new_checkpoint[f\"encoder.down_blocks.{i}.downsamplers.0.conv.weight\"] = vae_state_dict.pop(\n",
        "                f\"encoder.down.{i}.downsample.conv.weight\"\n",
        "            )\n",
        "            new_checkpoint[f\"encoder.down_blocks.{i}.downsamplers.0.conv.bias\"] = vae_state_dict.pop(\n",
        "                f\"encoder.down.{i}.downsample.conv.bias\"\n",
        "            )\n",
        "\n",
        "        paths = renew_vae_resnet_paths(resnets)\n",
        "        meta_path = {\"old\": f\"down.{i}.block\", \"new\": f\"down_blocks.{i}.resnets\"}\n",
        "        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
        "\n",
        "    mid_resnets = [key for key in vae_state_dict if \"encoder.mid.block\" in key]\n",
        "    num_mid_res_blocks = 2\n",
        "    for i in range(1, num_mid_res_blocks + 1):\n",
        "        resnets = [key for key in mid_resnets if f\"encoder.mid.block_{i}\" in key]\n",
        "\n",
        "        paths = renew_vae_resnet_paths(resnets)\n",
        "        meta_path = {\"old\": f\"mid.block_{i}\", \"new\": f\"mid_block.resnets.{i - 1}\"}\n",
        "        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
        "\n",
        "    mid_attentions = [key for key in vae_state_dict if \"encoder.mid.attn\" in key]\n",
        "    paths = renew_vae_attention_paths(mid_attentions)\n",
        "    meta_path = {\"old\": \"mid.attn_1\", \"new\": \"mid_block.attentions.0\"}\n",
        "    assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
        "    conv_attn_to_linear(new_checkpoint)\n",
        "\n",
        "    for i in range(num_up_blocks):\n",
        "        block_id = num_up_blocks - 1 - i\n",
        "        resnets = [\n",
        "            key for key in up_blocks[block_id] if f\"up.{block_id}\" in key and f\"up.{block_id}.upsample\" not in key\n",
        "        ]\n",
        "\n",
        "        if f\"decoder.up.{block_id}.upsample.conv.weight\" in vae_state_dict:\n",
        "            new_checkpoint[f\"decoder.up_blocks.{i}.upsamplers.0.conv.weight\"] = vae_state_dict[\n",
        "                f\"decoder.up.{block_id}.upsample.conv.weight\"\n",
        "            ]\n",
        "            new_checkpoint[f\"decoder.up_blocks.{i}.upsamplers.0.conv.bias\"] = vae_state_dict[\n",
        "                f\"decoder.up.{block_id}.upsample.conv.bias\"\n",
        "            ]\n",
        "\n",
        "        paths = renew_vae_resnet_paths(resnets)\n",
        "        meta_path = {\"old\": f\"up.{block_id}.block\", \"new\": f\"up_blocks.{i}.resnets\"}\n",
        "        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
        "\n",
        "    mid_resnets = [key for key in vae_state_dict if \"decoder.mid.block\" in key]\n",
        "    num_mid_res_blocks = 2\n",
        "    for i in range(1, num_mid_res_blocks + 1):\n",
        "        resnets = [key for key in mid_resnets if f\"decoder.mid.block_{i}\" in key]\n",
        "\n",
        "        paths = renew_vae_resnet_paths(resnets)\n",
        "        meta_path = {\"old\": f\"mid.block_{i}\", \"new\": f\"mid_block.resnets.{i - 1}\"}\n",
        "        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
        "\n",
        "    mid_attentions = [key for key in vae_state_dict if \"decoder.mid.attn\" in key]\n",
        "    paths = renew_vae_attention_paths(mid_attentions)\n",
        "    meta_path = {\"old\": \"mid.attn_1\", \"new\": \"mid_block.attentions.0\"}\n",
        "    assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
        "    conv_attn_to_linear(new_checkpoint)\n",
        "    return new_checkpoint\n",
        "\n",
        "\n",
        "CLAP_KEYS_TO_MODIFY_MAPPING = {\n",
        "    \"text_branch\": \"text_model\",\n",
        "    \"attn\": \"attention.self\",\n",
        "    \"self.proj\": \"output.dense\",\n",
        "    \"attention.self_mask\": \"attn_mask\",\n",
        "    \"mlp.fc1\": \"intermediate.dense\",\n",
        "    \"mlp.fc2\": \"output.dense\",\n",
        "    \"norm1\": \"layernorm_before\",\n",
        "    \"norm2\": \"layernorm_after\",\n",
        "    \"bn0\": \"batch_norm\",\n",
        "}\n",
        "\n",
        "CLAP_KEYS_TO_IGNORE = [\"text_transform\"]\n",
        "\n",
        "CLAP_EXPECTED_MISSING_KEYS = [\"text_model.embeddings.token_type_ids\"]\n",
        "\n",
        "\n",
        "def convert_open_clap_checkpoint(checkpoint):\n",
        "    \"\"\"\n",
        "    Takes a state dict and returns a converted CLAP checkpoint.\n",
        "    \"\"\"\n",
        "    # extract state dict for CLAP text embedding model, discarding the audio component\n",
        "    model_state_dict = {}\n",
        "    model_key = \"cond_stage_model.model.text_\"\n",
        "    keys = list(checkpoint.keys())\n",
        "    for key in keys:\n",
        "        if key.startswith(model_key):\n",
        "            model_state_dict[key.replace(model_key, \"text_\")] = checkpoint.get(key)\n",
        "\n",
        "    new_checkpoint = {}\n",
        "\n",
        "    sequential_layers_pattern = r\".*sequential.(\\d+).*\"\n",
        "    text_projection_pattern = r\".*_projection.(\\d+).*\"\n",
        "\n",
        "    for key, value in model_state_dict.items():\n",
        "        # check if key should be ignored in mapping\n",
        "        if key.split(\".\")[0] in CLAP_KEYS_TO_IGNORE:\n",
        "            continue\n",
        "\n",
        "        # check if any key needs to be modified\n",
        "        for key_to_modify, new_key in CLAP_KEYS_TO_MODIFY_MAPPING.items():\n",
        "            if key_to_modify in key:\n",
        "                key = key.replace(key_to_modify, new_key)\n",
        "\n",
        "        if re.match(sequential_layers_pattern, key):\n",
        "            # replace sequential layers with list\n",
        "            sequential_layer = re.match(sequential_layers_pattern, key).group(1)\n",
        "\n",
        "            key = key.replace(f\"sequential.{sequential_layer}.\", f\"layers.{int(sequential_layer)//3}.linear.\")\n",
        "        elif re.match(text_projection_pattern, key):\n",
        "            projecton_layer = int(re.match(text_projection_pattern, key).group(1))\n",
        "\n",
        "            # Because in CLAP they use `nn.Sequential`...\n",
        "            transformers_projection_layer = 1 if projecton_layer == 0 else 2\n",
        "\n",
        "            key = key.replace(f\"_projection.{projecton_layer}.\", f\"_projection.linear{transformers_projection_layer}.\")\n",
        "\n",
        "        if \"audio\" and \"qkv\" in key:\n",
        "            # split qkv into query key and value\n",
        "            mixed_qkv = value\n",
        "            qkv_dim = mixed_qkv.size(0) // 3\n",
        "\n",
        "            query_layer = mixed_qkv[:qkv_dim]\n",
        "            key_layer = mixed_qkv[qkv_dim : qkv_dim * 2]\n",
        "            value_layer = mixed_qkv[qkv_dim * 2 :]\n",
        "\n",
        "            new_checkpoint[key.replace(\"qkv\", \"query\")] = query_layer\n",
        "            new_checkpoint[key.replace(\"qkv\", \"key\")] = key_layer\n",
        "            new_checkpoint[key.replace(\"qkv\", \"value\")] = value_layer\n",
        "        else:\n",
        "            new_checkpoint[key] = value\n",
        "\n",
        "    return new_checkpoint\n",
        "\n",
        "\n",
        "def create_transformers_vocoder_config(original_config):\n",
        "    \"\"\"\n",
        "    Creates a config for transformers SpeechT5HifiGan based on the config of the vocoder model.\n",
        "    \"\"\"\n",
        "    vocoder_params = original_config.model.params.vocoder_config.params\n",
        "\n",
        "    config = {\n",
        "        \"model_in_dim\": vocoder_params.num_mels,\n",
        "        \"sampling_rate\": vocoder_params.sampling_rate,\n",
        "        \"upsample_initial_channel\": vocoder_params.upsample_initial_channel,\n",
        "        \"upsample_rates\": list(vocoder_params.upsample_rates),\n",
        "        \"upsample_kernel_sizes\": list(vocoder_params.upsample_kernel_sizes),\n",
        "        \"resblock_kernel_sizes\": list(vocoder_params.resblock_kernel_sizes),\n",
        "        \"resblock_dilation_sizes\": [\n",
        "            list(resblock_dilation) for resblock_dilation in vocoder_params.resblock_dilation_sizes\n",
        "        ],\n",
        "        \"normalize_before\": False,\n",
        "    }\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def convert_hifigan_checkpoint(checkpoint, config):\n",
        "    \"\"\"\n",
        "    Takes a state dict and config, and returns a converted HiFiGAN vocoder checkpoint.\n",
        "    \"\"\"\n",
        "    # extract state dict for vocoder\n",
        "    vocoder_state_dict = {}\n",
        "    vocoder_key = \"first_stage_model.vocoder.\"\n",
        "    keys = list(checkpoint.keys())\n",
        "    for key in keys:\n",
        "        if key.startswith(vocoder_key):\n",
        "            vocoder_state_dict[key.replace(vocoder_key, \"\")] = checkpoint.get(key)\n",
        "\n",
        "    # fix upsampler keys, everything else is correct already\n",
        "    for i in range(len(config.upsample_rates)):\n",
        "        vocoder_state_dict[f\"upsampler.{i}.weight\"] = vocoder_state_dict.pop(f\"ups.{i}.weight\")\n",
        "        vocoder_state_dict[f\"upsampler.{i}.bias\"] = vocoder_state_dict.pop(f\"ups.{i}.bias\")\n",
        "\n",
        "    if not config.normalize_before:\n",
        "        # if we don't set normalize_before then these variables are unused, so we set them to their initialised values\n",
        "        vocoder_state_dict[\"mean\"] = torch.zeros(config.model_in_dim)\n",
        "        vocoder_state_dict[\"scale\"] = torch.ones(config.model_in_dim)\n",
        "\n",
        "    return vocoder_state_dict\n",
        "\n",
        "\n",
        "# Adapted from https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation/blob/84a0384742a22bd80c44e903e241f0623e874f1d/audioldm/utils.py#L72-L73\n",
        "DEFAULT_CONFIG = {\n",
        "    \"model\": {\n",
        "        \"params\": {\n",
        "            \"linear_start\": 0.0015,\n",
        "            \"linear_end\": 0.0195,\n",
        "            \"timesteps\": 1000,\n",
        "            \"channels\": 8,\n",
        "            \"scale_by_std\": True,\n",
        "            \"unet_config\": {\n",
        "                \"target\": \"audioldm.latent_diffusion.openaimodel.UNetModel\",\n",
        "                \"params\": {\n",
        "                    \"extra_film_condition_dim\": 512,\n",
        "                    \"extra_film_use_concat\": True,\n",
        "                    \"in_channels\": 8,\n",
        "                    \"out_channels\": 8,\n",
        "                    \"model_channels\": 128,\n",
        "                    \"attention_resolutions\": [8, 4, 2],\n",
        "                    \"num_res_blocks\": 2,\n",
        "                    \"channel_mult\": [1, 2, 3, 5],\n",
        "                    \"num_head_channels\": 32,\n",
        "                },\n",
        "            },\n",
        "            \"first_stage_config\": {\n",
        "                \"target\": \"audioldm.variational_autoencoder.autoencoder.AutoencoderKL\",\n",
        "                \"params\": {\n",
        "                    \"embed_dim\": 8,\n",
        "                    \"ddconfig\": {\n",
        "                        \"z_channels\": 8,\n",
        "                        \"resolution\": 256,\n",
        "                        \"in_channels\": 1,\n",
        "                        \"out_ch\": 1,\n",
        "                        \"ch\": 128,\n",
        "                        \"ch_mult\": [1, 2, 4],\n",
        "                        \"num_res_blocks\": 2,\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "            \"vocoder_config\": {\n",
        "                \"target\": \"audioldm.first_stage_model.vocoder\",\n",
        "                \"params\": {\n",
        "                    \"upsample_rates\": [5, 4, 2, 2, 2],\n",
        "                    \"upsample_kernel_sizes\": [16, 16, 8, 4, 4],\n",
        "                    \"upsample_initial_channel\": 1024,\n",
        "                    \"resblock_kernel_sizes\": [3, 7, 11],\n",
        "                    \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
        "                    \"num_mels\": 64,\n",
        "                    \"sampling_rate\": 16000,\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def load_pipeline_from_original_audioldm_ckpt(\n",
        "    checkpoint_path: str,\n",
        "    original_config_file: str = None,\n",
        "    image_size: int = 512,\n",
        "    prediction_type: str = None,\n",
        "    extract_ema: bool = False,\n",
        "    scheduler_type: str = \"ddim\",\n",
        "    num_in_channels: int = None,\n",
        "    model_channels: int = None,\n",
        "    num_head_channels: int = None,\n",
        "    device: str = None,\n",
        "    from_safetensors: bool = False,\n",
        ") -> AudioLDMPipeline:\n",
        "    \"\"\"\n",
        "    Load an AudioLDM pipeline object from a `.ckpt`/`.safetensors` file and (ideally) a `.yaml` config file.\n",
        "\n",
        "    Although many of the arguments can be automatically inferred, some of these rely on brittle checks against the\n",
        "    global step count, which will likely fail for models that have undergone further fine-tuning. Therefore, it is\n",
        "    recommended that you override the default values and/or supply an `original_config_file` wherever possible.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path (`str`): Path to `.ckpt` file.\n",
        "        original_config_file (`str`):\n",
        "            Path to `.yaml` config file corresponding to the original architecture. If `None`, will be automatically\n",
        "            set to the audioldm-s-full-v2 config.\n",
        "        image_size (`int`, *optional*, defaults to 512):\n",
        "            The image size that the model was trained on.\n",
        "        prediction_type (`str`, *optional*):\n",
        "            The prediction type that the model was trained on. If `None`, will be automatically\n",
        "            inferred by looking for a key in the config. For the default config, the prediction type is `'epsilon'`.\n",
        "        num_in_channels (`int`, *optional*, defaults to None):\n",
        "            The number of UNet input channels. If `None`, it will be automatically inferred from the config.\n",
        "        model_channels (`int`, *optional*, defaults to None):\n",
        "            The number of UNet model channels. If `None`, it will be automatically inferred from the config. Override\n",
        "            to 128 for the small checkpoints, 192 for the medium checkpoints and 256 for the large.\n",
        "        num_head_channels (`int`, *optional*, defaults to None):\n",
        "            The number of UNet head channels. If `None`, it will be automatically inferred from the config. Override\n",
        "            to 32 for the small and medium checkpoints, and 64 for the large.\n",
        "        scheduler_type (`str`, *optional*, defaults to 'pndm'):\n",
        "            Type of scheduler to use. Should be one of `[\"pndm\", \"lms\", \"heun\", \"euler\", \"euler-ancestral\", \"dpm\",\n",
        "            \"ddim\"]`.\n",
        "        extract_ema (`bool`, *optional*, defaults to `False`): Only relevant for\n",
        "            checkpoints that have both EMA and non-EMA weights. Whether to extract the EMA weights or not. Defaults to\n",
        "            `False`. Pass `True` to extract the EMA weights. EMA weights usually yield higher quality images for\n",
        "            inference. Non-EMA weights are usually better to continue fine-tuning.\n",
        "        device (`str`, *optional*, defaults to `None`):\n",
        "            The device to use. Pass `None` to determine automatically.\n",
        "        from_safetensors (`str`, *optional*, defaults to `False`):\n",
        "            If `checkpoint_path` is in `safetensors` format, load checkpoint with safetensors instead of PyTorch.\n",
        "        return: An AudioLDMPipeline object representing the passed-in `.ckpt`/`.safetensors` file.\n",
        "    \"\"\"\n",
        "\n",
        "    if not is_omegaconf_available():\n",
        "        raise ValueError(BACKENDS_MAPPING[\"omegaconf\"][1])\n",
        "\n",
        "    from omegaconf import OmegaConf\n",
        "\n",
        "    if from_safetensors:\n",
        "        if not is_safetensors_available():\n",
        "            raise ValueError(BACKENDS_MAPPING[\"safetensors\"][1])\n",
        "\n",
        "        from safetensors import safe_open\n",
        "\n",
        "        checkpoint = {}\n",
        "        with safe_open(checkpoint_path, framework=\"pt\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                checkpoint[key] = f.get_tensor(key)\n",
        "    else:\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        else:\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    if \"state_dict\" in checkpoint:\n",
        "        checkpoint = checkpoint[\"state_dict\"]\n",
        "\n",
        "    if original_config_file is None:\n",
        "        original_config = DEFAULT_CONFIG\n",
        "        original_config = OmegaConf.create(original_config)\n",
        "    else:\n",
        "        original_config = OmegaConf.load(original_config_file)\n",
        "\n",
        "    if num_in_channels is not None:\n",
        "        original_config[\"model\"][\"params\"][\"unet_config\"][\"params\"][\"in_channels\"] = num_in_channels\n",
        "\n",
        "    if model_channels is not None:\n",
        "        original_config[\"model\"][\"params\"][\"unet_config\"][\"params\"][\"model_channels\"] = model_channels\n",
        "\n",
        "    if num_head_channels is not None:\n",
        "        original_config[\"model\"][\"params\"][\"unet_config\"][\"params\"][\"num_head_channels\"] = num_head_channels\n",
        "\n",
        "    if (\n",
        "        \"parameterization\" in original_config[\"model\"][\"params\"]\n",
        "        and original_config[\"model\"][\"params\"][\"parameterization\"] == \"v\"\n",
        "    ):\n",
        "        if prediction_type is None:\n",
        "            prediction_type = \"v_prediction\"\n",
        "    else:\n",
        "        if prediction_type is None:\n",
        "            prediction_type = \"epsilon\"\n",
        "\n",
        "    if image_size is None:\n",
        "        image_size = 512\n",
        "\n",
        "    num_train_timesteps = original_config.model.params.timesteps\n",
        "    beta_start = original_config.model.params.linear_start\n",
        "    beta_end = original_config.model.params.linear_end\n",
        "\n",
        "    scheduler = DDIMScheduler(\n",
        "        beta_end=beta_end,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        beta_start=beta_start,\n",
        "        num_train_timesteps=num_train_timesteps,\n",
        "        steps_offset=1,\n",
        "        clip_sample=False,\n",
        "        set_alpha_to_one=False,\n",
        "        prediction_type=prediction_type,\n",
        "    )\n",
        "    # make sure scheduler works correctly with DDIM\n",
        "    scheduler.register_to_config(clip_sample=False)\n",
        "\n",
        "    if scheduler_type == \"pndm\":\n",
        "        config = dict(scheduler.config)\n",
        "        config[\"skip_prk_steps\"] = True\n",
        "        scheduler = PNDMScheduler.from_config(config)\n",
        "    elif scheduler_type == \"lms\":\n",
        "        scheduler = LMSDiscreteScheduler.from_config(scheduler.config)\n",
        "    elif scheduler_type == \"heun\":\n",
        "        scheduler = HeunDiscreteScheduler.from_config(scheduler.config)\n",
        "    elif scheduler_type == \"euler\":\n",
        "        scheduler = EulerDiscreteScheduler.from_config(scheduler.config)\n",
        "    elif scheduler_type == \"euler-ancestral\":\n",
        "        scheduler = EulerAncestralDiscreteScheduler.from_config(scheduler.config)\n",
        "    elif scheduler_type == \"dpm\":\n",
        "        scheduler = DPMSolverMultistepScheduler.from_config(scheduler.config)\n",
        "    elif scheduler_type == \"ddim\":\n",
        "        scheduler = scheduler\n",
        "    else:\n",
        "        raise ValueError(f\"Scheduler of type {scheduler_type} doesn't exist!\")\n",
        "\n",
        "    # Convert the UNet2DModel\n",
        "    unet_config = create_unet_diffusers_config(original_config, image_size=image_size)\n",
        "    unet = UNet2DConditionModel(**unet_config)\n",
        "\n",
        "    converted_unet_checkpoint = convert_ldm_unet_checkpoint(\n",
        "        checkpoint, unet_config, path=checkpoint_path, extract_ema=extract_ema\n",
        "    )\n",
        "\n",
        "    unet.load_state_dict(converted_unet_checkpoint)\n",
        "\n",
        "    # Convert the VAE model\n",
        "    vae_config = create_vae_diffusers_config(original_config, checkpoint=checkpoint, image_size=image_size)\n",
        "    converted_vae_checkpoint = convert_ldm_vae_checkpoint(checkpoint, vae_config)\n",
        "\n",
        "    vae = AutoencoderKL(**vae_config)\n",
        "    vae.load_state_dict(converted_vae_checkpoint)\n",
        "\n",
        "    # Convert the text model\n",
        "    # AudioLDM uses the same configuration and tokenizer as the original CLAP model\n",
        "    config = ClapTextConfig.from_pretrained(\"laion/clap-htsat-unfused\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n",
        "\n",
        "    converted_text_model = convert_open_clap_checkpoint(checkpoint)\n",
        "    text_model = ClapTextModelWithProjection(config)\n",
        "\n",
        "    missing_keys, unexpected_keys = text_model.load_state_dict(converted_text_model, strict=False)\n",
        "    # we expect not to have token_type_ids in our original state dict so let's ignore them\n",
        "    missing_keys = list(set(missing_keys) - set(CLAP_EXPECTED_MISSING_KEYS))\n",
        "\n",
        "    if len(unexpected_keys) > 0:\n",
        "        raise ValueError(f\"Unexpected keys when loading CLAP model: {unexpected_keys}\")\n",
        "\n",
        "    if len(missing_keys) > 0:\n",
        "        raise ValueError(f\"Missing keys when loading CLAP model: {missing_keys}\")\n",
        "\n",
        "    # Convert the vocoder model\n",
        "    vocoder_config = create_transformers_vocoder_config(original_config)\n",
        "    vocoder_config = SpeechT5HifiGanConfig(**vocoder_config)\n",
        "    converted_vocoder_checkpoint = convert_hifigan_checkpoint(checkpoint, vocoder_config)\n",
        "\n",
        "    vocoder = SpeechT5HifiGan(vocoder_config)\n",
        "    vocoder.load_state_dict(converted_vocoder_checkpoint)\n",
        "\n",
        "    # Instantiate the diffusers pipeline\n",
        "    pipe = AudioLDMPipeline(\n",
        "        vae=vae,\n",
        "        text_encoder=text_model,\n",
        "        tokenizer=tokenizer,\n",
        "        unet=unet,\n",
        "        scheduler=scheduler,\n",
        "        vocoder=vocoder,\n",
        "    )\n",
        "\n",
        "    return pipe\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--original_config_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The YAML config file corresponding to the original architecture.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_in_channels\",\n",
        "        default=None,\n",
        "        type=int,\n",
        "        help=\"The number of input channels. If `None` number of input channels will be automatically inferred.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_channels\",\n",
        "        default=None,\n",
        "        type=int,\n",
        "        help=\"The number of UNet model channels. If `None`, it will be automatically inferred from the config. Override\"\n",
        "        \" to 128 for the small checkpoints, 192 for the medium checkpoints and 256 for the large.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_head_channels\",\n",
        "        default=None,\n",
        "        type=int,\n",
        "        help=\"The number of UNet head channels. If `None`, it will be automatically inferred from the config. Override\"\n",
        "        \" to 32 for the small and medium checkpoints, and 64 for the large.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--scheduler_type\",\n",
        "        default=\"ddim\",\n",
        "        type=str,\n",
        "        help=\"Type of scheduler to use. Should be one of ['pndm', 'lms', 'ddim', 'euler', 'euler-ancestral', 'dpm']\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--image_size\",\n",
        "        default=None,\n",
        "        type=int,\n",
        "        help=(\"The image size that the model was trained on.\"),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--prediction_type\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=(\"The prediction type that the model was trained on.\"),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--extract_ema\",\n",
        "        action=\"store_true\",\n",
        "        help=(\n",
        "            \"Only relevant for checkpoints that have both EMA and non-EMA weights. Whether to extract the EMA weights\"\n",
        "            \" or not. Defaults to `False`. Add `--extract_ema` to extract the EMA weights. EMA weights usually yield\"\n",
        "            \" higher quality images for inference. Non-EMA weights are usually better to continue fine-tuning.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--from_safetensors\",\n",
        "        action=\"store_true\",\n",
        "        help=\"If `--checkpoint_path` is in `safetensors` format, load checkpoint with safetensors instead of PyTorch.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--to_safetensors\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to store pipeline in safetensors format or not.\",\n",
        "    )\n",
        "    parser.add_argument(\"--dump_path\", default=None, type=str, required=True, help=\"Path to the output model.\")\n",
        "    parser.add_argument(\"--device\", type=str, help=\"Device to use (e.g. cpu, cuda:0, cuda:1, etc.)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    pipe = load_pipeline_from_original_audioldm_ckpt(\n",
        "        checkpoint_path=args.checkpoint_path,\n",
        "        original_config_file=args.original_config_file,\n",
        "        image_size=args.image_size,\n",
        "        prediction_type=args.prediction_type,\n",
        "        extract_ema=args.extract_ema,\n",
        "        scheduler_type=args.scheduler_type,\n",
        "        num_in_channels=args.num_in_channels,\n",
        "        model_channels=args.model_channels,\n",
        "        num_head_channels=args.num_head_channels,\n",
        "        from_safetensors=args.from_safetensors,\n",
        "        device=args.device,\n",
        "    )\n",
        "    pipe.save_pretrained(args.dump_path, safe_serialization=args.to_safetensors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6evwsznQizy",
        "outputId": "209ccfe2-ebff-4e23-f878-68176ae8a2e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing convert.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert.py --checkpoint_path=\"musicldm-test/musicldm-ckpt.ckpt\" --dump_path=\"\" --to_safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAExJ_rvQob8",
        "outputId": "154a7885-a973-40b5-bc7a-6986c38e3280"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-08 08:38:04.888537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "In this conversion only the non-EMA weights are extracted. If you want to instead extract the EMA weights (usually better for inference), please make sure to add the `--extract_ema` flag.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/convert.py\", line 1040, in <module>\n",
            "    pipe = load_pipeline_from_original_audioldm_ckpt(\n",
            "  File \"/content/convert.py\", line 926, in load_pipeline_from_original_audioldm_ckpt\n",
            "    vae.load_state_dict(converted_vae_checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for AutoencoderKL:\n",
            "\tMissing key(s) in state_dict: \"encoder.mid_block.attentions.0.to_q.weight\", \"encoder.mid_block.attentions.0.to_q.bias\", \"encoder.mid_block.attentions.0.to_k.weight\", \"encoder.mid_block.attentions.0.to_k.bias\", \"encoder.mid_block.attentions.0.to_v.weight\", \"encoder.mid_block.attentions.0.to_v.bias\", \"encoder.mid_block.attentions.0.to_out.0.weight\", \"encoder.mid_block.attentions.0.to_out.0.bias\", \"decoder.mid_block.attentions.0.to_q.weight\", \"decoder.mid_block.attentions.0.to_q.bias\", \"decoder.mid_block.attentions.0.to_k.weight\", \"decoder.mid_block.attentions.0.to_k.bias\", \"decoder.mid_block.attentions.0.to_v.weight\", \"decoder.mid_block.attentions.0.to_v.bias\", \"decoder.mid_block.attentions.0.to_out.0.weight\", \"decoder.mid_block.attentions.0.to_out.0.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"encoder.mid_block.attentions.0.query.weight\", \"encoder.mid_block.attentions.0.query.bias\", \"encoder.mid_block.attentions.0.key.weight\", \"encoder.mid_block.attentions.0.key.bias\", \"encoder.mid_block.attentions.0.value.weight\", \"encoder.mid_block.attentions.0.value.bias\", \"encoder.mid_block.attentions.0.proj_attn.weight\", \"encoder.mid_block.attentions.0.proj_attn.bias\", \"decoder.mid_block.attentions.0.query.weight\", \"decoder.mid_block.attentions.0.query.bias\", \"decoder.mid_block.attentions.0.key.weight\", \"decoder.mid_block.attentions.0.key.bias\", \"decoder.mid_block.attentions.0.value.weight\", \"decoder.mid_block.attentions.0.value.bias\", \"decoder.mid_block.attentions.0.proj_attn.weight\", \"decoder.mid_block.attentions.0.proj_attn.bias\". \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AudioLDMPipeline\n",
        "import torch\n",
        "import scipy\n",
        "\n",
        "repo_id = \"cvssp/audioldm-s-full-v2\"\n",
        "pipe = AudioLDMPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"Techno music with a strong, upbeat tempo and high melodic riffs\"\n",
        "audio = pipe(prompt, num_inference_steps=10, audio_length_in_s=5.0).audios[0]"
      ],
      "metadata": {
        "id": "G_DBDVocGbgr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}