{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCzW3Qn5fbr+vgH3To0p+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhavs10/scratchpad/blob/main/UNet_error.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CxRz9HkJmaP7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic UNet works"
      ],
      "metadata": {
        "id": "WqFe6EDZm_aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminatorNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.up = nn.ModuleList([\n",
        "            nn.utils.weight_norm(nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=2)),\n",
        "        ])\n",
        "\n",
        "        self.up_out = nn.utils.weight_norm(nn.Conv2d(256, 1, kernel_size=3, stride=1))\n",
        "\n",
        "        self.down = nn.ModuleList([\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(256, 64, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(128, 32, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(64, 16, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(32, 8, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=2)),\n",
        "        ])\n",
        "        \n",
        "        self.down_out = nn.utils.weight_norm(nn.Conv2d(1, 1, kernel_size=3, stride=1))\n",
        "\n",
        "        self.fc = nn.Linear(8000, 1)  # this needs to be changed everytime the window length is changes. It would be nice if this could be done dynamically.\n",
        "\n",
        "    def forward(self, y):\n",
        "        upsample_outputs = list()\n",
        "        feature_maps = list()\n",
        "        for d_up in self.up:\n",
        "            y = d_up(y)\n",
        "            y = nn.functional.leaky_relu(y, 0.2)\n",
        "            upsample_outputs.append(y) \n",
        "\n",
        "        up_f_map = self.up_out(y)\n",
        "        feature_maps.append(up_f_map)\n",
        "\n",
        "        counter = 0\n",
        "        uo_len = len(upsample_outputs)\n",
        "        for d_down in self.down:\n",
        "            print(d_down)\n",
        "            if counter == 0:\n",
        "                y = d_down(y)\n",
        "                y = nn.functional.leaky_relu(y, 0.2)\n",
        "            if counter >= 1:\n",
        "                print(y.shape)\n",
        "                print(upsample_outputs[uo_len - counter - 1].shape)\n",
        "                _ = torch.cat((y, upsample_outputs[uo_len - counter -1]), dim=1)\n",
        "                print(_.shape)\n",
        "                y = d_down(torch.cat((y, upsample_outputs[uo_len - counter - 1]), dim=1))\n",
        "                y = nn.functional.leaky_relu(y, 0.2)\n",
        "            counter+=1\n",
        "        \n",
        "        down_f_map = self.down_out(y)\n",
        "        feature_maps.append(down_f_map)\n",
        "        \n",
        "        y = torch.flatten(y, 1, -1)\n",
        "        y = self.fc(y)\n",
        "\n",
        "        return y, feature_maps"
      ],
      "metadata": {
        "id": "ZJqDrltcmgtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake = torch.randn([2, 100, 80])  # [Batch, Sequence Length, Spectrogram Buckets]\n",
        "real = torch.randn([2, 100, 80])\n",
        "\n",
        "d = DiscriminatorNet()\n",
        "y, fmaps = d(fake.unsqueeze(1))"
      ],
      "metadata": {
        "id": "5qLjs9xcm6C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updating the stride + kernel size borks it"
      ],
      "metadata": {
        "id": "N_bk4Fy1nDKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminatorNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.up = nn.ModuleList([\n",
        "            nn.utils.weight_norm(nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=2)),\n",
        "        ])\n",
        "\n",
        "        self.up_out = nn.utils.weight_norm(nn.Conv2d(256, 1, kernel_size=3, stride=1))\n",
        "\n",
        "        self.down = nn.ModuleList([\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(256, 64, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(64, 16, kernel_size=3, stride=1, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(32, 8, kernel_size=4, stride=2, padding=2)),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=2)),\n",
        "        ])\n",
        "        \n",
        "        self.down_out = nn.utils.weight_norm(nn.Conv2d(1, 1, kernel_size=3, stride=1))\n",
        "\n",
        "        self.fc = nn.Linear(8000, 1)  # this needs to be changed everytime the window length is changes. It would be nice if this could be done dynamically.\n",
        "\n",
        "    def forward(self, y):\n",
        "        upsample_outputs = list()\n",
        "        feature_maps = list()\n",
        "        for d_up in self.up:\n",
        "            y = d_up(y)\n",
        "            y = nn.functional.leaky_relu(y, 0.2)\n",
        "            upsample_outputs.append(y) \n",
        "\n",
        "        up_f_map = self.up_out(y)\n",
        "        feature_maps.append(up_f_map)\n",
        "\n",
        "        counter = 0\n",
        "        uo_len = len(upsample_outputs)\n",
        "        for d_down in self.down:\n",
        "            print(d_down)\n",
        "            if counter == 0:\n",
        "                y = d_down(y)\n",
        "                y = nn.functional.leaky_relu(y, 0.2)\n",
        "            if counter >= 1:\n",
        "                print(y.shape)\n",
        "                print(upsample_outputs[uo_len - counter - 1].shape)\n",
        "                _ = torch.cat((y, upsample_outputs[uo_len - counter -1]), dim=1)\n",
        "                print(_.shape)\n",
        "                y = d_down(torch.cat((y, upsample_outputs[uo_len - counter - 1]), dim=1))\n",
        "                y = nn.functional.leaky_relu(y, 0.2)\n",
        "            counter+=1\n",
        "        \n",
        "        down_f_map = self.down_out(y)\n",
        "        feature_maps.append(down_f_map)\n",
        "        \n",
        "        y = torch.flatten(y, 1, -1)\n",
        "        y = self.fc(y)\n",
        "\n",
        "        return y, feature_maps"
      ],
      "metadata": {
        "id": "GEtNlnmOnCaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake = torch.randn([2, 100, 80])  # [Batch, Sequence Length, Spectrogram Buckets]\n",
        "real = torch.randn([2, 100, 80])\n",
        "\n",
        "d = DiscriminatorNet()\n",
        "y, fmaps = d(fake.unsqueeze(1))"
      ],
      "metadata": {
        "id": "53hBoBDwnfBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}